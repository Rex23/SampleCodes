{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created on Wed Oct 24 16:19:49 2018\n",
      "\n",
      "@author: xiang\n",
      "\n",
      "\f",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\xiang\\documents\\xiang\\venv_new\\lib\\site-packages\\sklearn\\preprocessing\\data.py:176: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "c:\\users\\xiang\\documents\\xiang\\venv_new\\lib\\site-packages\\sklearn\\preprocessing\\data.py:193: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Oct 24 16:19:49 2018\n",
    "\n",
    "@author: xiang\n",
    "\"\"\"\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "from IPython import get_ipython\n",
    "get_ipython().magic('reset -sf')\n",
    "get_ipython().magic('clear')\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import neighbors, preprocessing\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score, ShuffleSplit\n",
    "from time import time\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "filename = 'Letter.csv'\n",
    "data = np.genfromtxt(filename, delimiter=',', dtype='str')\n",
    "sample_data = data[:,1:].astype(np.float32)\n",
    "num_of_features = sample_data.shape[1];\n",
    "sample_data = scale(sample_data)\n",
    "\n",
    "sample_target = data[:,0]\n",
    "#Transform the labels\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(sample_target.ravel())\n",
    "sample_target = le.transform(sample_target.ravel()).reshape(sample_target.shape) \n",
    "\n",
    "labels = np.unique(sample_target)\n",
    "num_labels = labels.size\n",
    "sample_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"n_digits: %d, \\t n_samples %d, \\t n_features %d\" % (num_labels, sample_data.shape[0], num_of_features))\n",
    "\n",
    "print(82 * '_')\n",
    "print('init\\t\\ttime\\tinertia\\thomo\\tcompl\\tv-meas\\tARI\\tAMI\\tsilhouette')\n",
    "\n",
    "def k_means(estimator, name, data):\n",
    "    t0 = time()\n",
    "    estimator.fit(data)\n",
    "    #print(metrics.homogeneity_score(sample_target, estimator.labels_))\n",
    "    \n",
    "    print('%-9s\\t%.2fs\\t%i\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f'\n",
    "          % (name, (time() - t0), estimator.inertia_,\n",
    "             metrics.homogeneity_score(sample_target, estimator.labels_),\n",
    "             metrics.completeness_score(sample_target, estimator.labels_),\n",
    "             metrics.v_measure_score(sample_target, estimator.labels_),\n",
    "             metrics.adjusted_rand_score(sample_target, estimator.labels_),\n",
    "             metrics.adjusted_mutual_info_score(sample_target,  estimator.labels_),\n",
    "             metrics.silhouette_score(data, estimator.labels_,\n",
    "                                      metric='euclidean',\n",
    "                                      sample_size=sample_size)))\n",
    "\n",
    "estimator = KMeans(init='k-means++', n_clusters=num_labels, n_init=10)\n",
    "k_means(estimator = estimator,name=\"k-means++\", data=sample_data)\n",
    "\n",
    "label_comparison = np.c_[sample_target, estimator.labels_]\n",
    "label_comparison2 = label_comparison[label_comparison[:, 0].argsort()]\n",
    "\n",
    "#estimator.predict(sample_data)\n",
    "\n",
    "#k_means(KMeans(init='random', n_clusters=num_labels, n_init=10),\n",
    "#              name=\"random\", data=sample_data)\n",
    "\n",
    "# in this case the seeding of the centers is deterministic, hence we run the\n",
    "# kmeans algorithm only once with n_init=1\n",
    "#min(num_labels,num_of_features)\n",
    "#pca = PCA(n_components='mle').fit(sample_data)\n",
    "#k_means(KMeans(init=pca.components_, n_clusters=num_labels, n_init=1),\n",
    "#               name=\"PCA-based\",\n",
    "#               data=sample_data)\n",
    "\n",
    "print(82 * '_')\n",
    "\n",
    "reduced_data = PCA(n_components=2).fit_transform(sample_data)\n",
    "kmeans = KMeans(init='k-means++', n_clusters=num_labels, n_init=10)\n",
    "kmeans.fit(reduced_data)\n",
    "\n",
    "# Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "h = .02     # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Obtain labels for each point in mesh. Use last trained model.\n",
    "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "plt.imshow(Z, interpolation='nearest',\n",
    "           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "           cmap=plt.cm.Paired,\n",
    "           aspect='auto', origin='lower')\n",
    "\n",
    "plt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=2)\n",
    "# Plot the centroids as a white X\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "            marker='x', s=169, linewidths=3,\n",
    "            color='w', zorder=10)\n",
    "plt.title('K-means clustering on the digits dataset (PCA-reduced data)\\n'\n",
    "          'Centroids are marked with white cross')\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
